{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "udtzE0HJ41JY"
      },
      "outputs": [],
      "source": [
        "#Importing the necessary Libraries\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjVFysnl47nA",
        "outputId": "74a7588c-fe66-4e43-ede7-f01fbd9416db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "filepath = tf.keras.utils.get_file('shakespear.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "text = open(filepath, 'rb').read().decode(encoding = 'utf-8').lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rgXj7uXu7CN4"
      },
      "outputs": [],
      "source": [
        "# #Selecting the characters that will be used in training to reduce training time\n",
        "\n",
        "# text = text[100000:1000000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VJcmuVLu7wLD"
      },
      "outputs": [],
      "source": [
        "#Specifying the characters\n",
        "\n",
        "characters = sorted(set(text)) #Set identifies the unique characters, sorted arranges them in alphabetical order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o7FfHQok6XLT"
      },
      "outputs": [],
      "source": [
        "#Converting the text to Numerical format and vice versa using dictionaries\n",
        "\n",
        "char_to_index = dict((c, i) for i, c in enumerate(characters))\n",
        "\n",
        "#i.e {'a':1, 'b':2, 'c':3 etc}\n",
        "\n",
        "index_to_char = dict((i, c) for i, c in enumerate(characters))\n",
        "\n",
        "#i.e {1:'a', 2:'b', 3:'c' etc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y9QGoKryAC_y"
      },
      "outputs": [],
      "source": [
        "#Character prediction\n",
        "\n",
        "SEQ_LEN = 40 # We will predict the next character based on the previous 40 characters\n",
        "STEP_SIZE = 3\n",
        "\n",
        "#Creating an empty list of the sentences and next charcters\n",
        "\n",
        "sentences = []\n",
        "next_characters = []\n",
        "\n",
        "for i in range(0, len(text)- SEQ_LEN, STEP_SIZE):\n",
        "  sentences.append(text[i: i+SEQ_LEN])\n",
        "  next_characters.append(text[i+SEQ_LEN])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GPvdM18XA-RQ"
      },
      "outputs": [],
      "source": [
        "#Converting the sentences and next characters generted into numpy arrays\n",
        "\n",
        "x = np.zeros((len(sentences), SEQ_LEN, len(characters)), dtype = bool)\n",
        "y = np.zeros((len(sentences), len(characters)), dtype = bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PiiO47glDaIj"
      },
      "outputs": [],
      "source": [
        "#Filling the two arrays using for loops\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, character in enumerate(sentence):\n",
        "      x[i,t,char_to_index[character]] = 1\n",
        "  y[i, char_to_index[next_characters[i]]] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb1A7D0IF38D",
        "outputId": "cc4320e3-53cd-4d94-f555-6f169a3ac1f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 2.2269\n",
            "Epoch 2/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - loss: 1.6074\n",
            "Epoch 3/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - loss: 1.5094\n",
            "Epoch 4/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 1.4572\n",
            "Epoch 5/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 1.4290\n",
            "Epoch 6/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 1.4032\n",
            "Epoch 7/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 1.3886\n",
            "Epoch 8/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 1.3742\n",
            "Epoch 9/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 1.3609\n",
            "Epoch 10/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 1.3506\n",
            "Epoch 11/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 1.3451\n",
            "Epoch 12/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 1.3400\n",
            "Epoch 13/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 1.3331\n",
            "Epoch 14/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 1.3272\n",
            "Epoch 15/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - loss: 1.3219\n",
            "Epoch 16/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - loss: 1.3175\n",
            "Epoch 17/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 1.3161\n",
            "Epoch 18/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - loss: 1.3120\n",
            "Epoch 19/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 1.3077\n",
            "Epoch 20/20\n",
            "\u001b[1m1453/1453\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 1.2998\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ec64a41b150>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Building the Recurrent Neural Network\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(128, input_shape = (SEQ_LEN, len(characters))))\n",
        "model.add(Dense(len(characters)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = RMSprop(learning_rate = 0.01))\n",
        "\n",
        "model.fit(x, y, batch_size = 256, epochs = 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y91vvyPcaRCE"
      },
      "outputs": [],
      "source": [
        "#Saving the model\n",
        "\n",
        "model.save('text_generator.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "flIgeQ_EN-Tc"
      },
      "outputs": [],
      "source": [
        "#Loading the model\n",
        "\n",
        "model = tf.keras.models.load_model('generator.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YmG2T7ciMPJL"
      },
      "outputs": [],
      "source": [
        "#making predictions of the next character and converting them to text\n",
        "\n",
        "def sample(preds, temperature = 1.0):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds)/temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds/np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1,preds,1)\n",
        "  return np.argmax(probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xexvbTBvQmrZ",
        "outputId": "6c50f6a1-4d55-4b3d-ea55-716aab5aa7d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------0.2-------------\n",
            "ne eyes.\n",
            "therefore, no more but this: he hath the bears,\n",
            "and the sensenion of the country's blood\n",
            "and the state and the prince since it is a beat of the court\n",
            "of the head that is the contratted of the heads\n",
            "and the peace and the man to the country\n",
            "of the state and she a presence and the state\n",
            "of the counterkelse that is the senseness of t\n",
            "\n",
            "-----------0.4-------------\n",
            "s daughter fairer than she is,\n",
            "she may make him as the rose of the villain.\n",
            "\n",
            "king richard iii:\n",
            "the counsels of the court, the strives here is a man are the fair\n",
            "women out the counsels of his condick to be the counsell haste thee,\n",
            "that they are by the manifes of my though here all the accused,\n",
            "and thou shalt be thou art the for the land\n",
            "th\n",
            "\n",
            "-----------0.6-------------\n",
            "nge his spots: take but my shame.\n",
            "and i will be well for the issue and the country:\n",
            "and that we shall be done on the friends.\n",
            "\n",
            "aufidius:\n",
            "my lord:\n",
            "the world of my fire and bread the stirned be both.\n",
            "\n",
            "lady anne:\n",
            "well, infemned her lord words am much comes.\n",
            "\n",
            "second murderer:\n",
            "you shall pay my presence the roseing your soul as the maid\n",
            "that we\n",
            "\n",
            "-----------0.8-------------\n",
            "or, thou wouldst have me answer to.\n",
            "\n",
            "queen margaret:\n",
            "feash only, my lord, which he gone the traitor,\n",
            "sir, thy bold men by me to gone.\n",
            "\n",
            "volumnia:\n",
            "if it were my perguace to death was clarence,\n",
            "thanken cradelal eam. impution, are you can take those though\n",
            "as our condity englort'd from my knee is\n",
            "count, i usermy; been such envious lord,\n",
            "even \n",
            "\n",
            "-----------1.0-------------\n",
            "king.\n",
            "\n",
            "autolycus:\n",
            "what advocate hast thou against here a petruchio:\n",
            "i'll ?\n",
            "is aumerle:\n",
            "who not a, inson fast\n",
            "from revenge, that takestasted you to the strength.\n",
            "\n",
            "mercutio:\n",
            "so comes fater.\n",
            "\n",
            "thirn usatr:\n",
            "i'll much unto guard wone, thest am re\n",
            "king men their crown as that confessed thee royalties herself;\n",
            "the precious back but god father wer\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Function to generate the text\n",
        "\n",
        "def generate_text(length, temperature):\n",
        "  start_index = random.randint(0, len(text) - SEQ_LEN - 1)\n",
        "  generated = ''\n",
        "  sentence = text[start_index: start_index +SEQ_LEN]\n",
        "  generated += sentence\n",
        "  for i in range(length):\n",
        "    x = np.zeros((1, SEQ_LEN, len(characters)))\n",
        "    for t, character in enumerate(sentence):\n",
        "        x[0,t, char_to_index[character]] = 1\n",
        "\n",
        "    predictions = model.predict(x, verbose = 0)[0]\n",
        "    next_index = sample(predictions, temperature)\n",
        "    next_character = index_to_char[next_index] # Converting the index to the next character\n",
        "\n",
        "\n",
        "    generated = generated+next_character\n",
        "    sentence = sentence[1:] + next_character\n",
        "  return generated\n",
        "\n",
        "print('-----------0.2-------------')\n",
        "print(generate_text(300, 0.2))\n",
        "print('')\n",
        "\n",
        "print('-----------0.4-------------')\n",
        "print(generate_text(300, 0.4))\n",
        "print('')\n",
        "\n",
        "print('-----------0.6-------------')\n",
        "print(generate_text(300, 0.6))\n",
        "print('')\n",
        "\n",
        "print('-----------0.8-------------')\n",
        "print(generate_text(300, 0.8))\n",
        "print('')\n",
        "\n",
        "print('-----------1.0-------------')\n",
        "print(generate_text(300, 1.0))\n",
        "print('')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
